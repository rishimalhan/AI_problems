# -*- coding: utf-8 -*-
"""A3_Rishi_Malhan.ipnyb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HCXTPSr6Oa1XP8XJ6UgCIGKz3DAee-d8

**Assignment-3**
Rishi Malhan
rmalhan@usc.edu
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

distribution = tf.keras.initializers.RandomUniform( minval=-1, maxval=1, seed=None )
def construct_classifier():
  model = tf.keras.Sequential()
  model.add(   tf.keras.layers.Conv2D(
                                32, (3,3), strides=(1, 1), padding='same',
                                input_shape=(28, 28, 1),
                                use_bias=True, activation='relu',
                                kernel_initializer=distribution,
                                bias_initializer=distribution,
                            ) )
  
  model.add(    tf.keras.layers.MaxPooling2D((2, 2)
                ) )  

  model.add(   tf.keras.layers.Conv2D(
                                64, (3,3), strides=(1, 1), padding='same',
                                use_bias=True, activation='relu',
                                kernel_initializer=distribution,
                                bias_initializer=distribution,
                            ) )
  
  model.add(    tf.keras.layers.MaxPooling2D((2, 2)
                ) )  
  
  model.add(   tf.keras.layers.Conv2D(
                                64, (3,3), strides=(1, 1), padding='valid',
                                use_bias=True, activation='relu',
                                kernel_initializer=distribution,
                                bias_initializer=distribution,
                            ) )
  
  model.add( tf.keras.layers.Flatten() )
  model.add(   tf.keras.layers.Dense(64, use_bias=True, activation='relu',
                                kernel_initializer=distribution,
                                bias_initializer=distribution,
                              ) )
  
  model.add(   tf.keras.layers.Dense(10, activation='softmax'
                              ) )
  
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']
              )
  return model

if __name__=='__main__':
  # In this assignment, you will train a deep convolutional network to classify images of hand-written digits.
  # The primary objective of this assignment is to learn how to build and train a CNN for image processing
  # tasks and visualize the learned filters.
  # 1. For this assignment we will be using MNIST dataset, which is a dataset of 60,000 gray-scale images of
  # hand-written digits of size 28 × 28.
  # 2. Download these images and re-scale them to have values between −1 and 1. For downloading and
  # splitting these into a training and testing set use the command: datasets.mnist.load data
  (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data(path='mnist.npz')
  # 3. Plot 25 training images in a 5 × 5 grid.
  print("Plotting 25 Images")
  plt.rcParams["figure.figsize"] = (15,10)
  fig1,axs1 = plt.subplots(5,5)
  for i in range(5):
    for j in range(5):
      axs1[i][j].imshow( train_images[np.random.choice(np.arange(0,train_images.shape[0]))] )
      axs1[i][j].axis("off")
      
  print("\n\n")
  


  # Normalize pixel values to be between 0 and 1
  train_images, test_images = train_images / 127.5, test_images / 127.5
  train_images, test_images = train_images - 1.0, test_images - 1.0

  train_images = train_images.reshape(list(train_images.shape) + [1])    # (60000, 28, 28, 1)
  test_images = test_images.reshape(list(test_images.shape) + [1])    # (10000, 28, 28, 1)

  # 4. Then construct a CNN based classifier with the following architecture (in all convolution and fully
  #   connected layers use ReLU activation except in the last fully-connected layer):
  #   • Convolution layer with 32 (3 × 3) filters and zero padding.
  #   • A 2 × 2 Max. pooling layer.
  #   • Convolution layer with 64 (3 × 3) filters with zero padding.
  #   • A 2 × 2 Max. pooling layer.
  #   • Convolution layer with 64 (3 × 3) filters with no padding.
  #   • Flatten the output of the previous layer and connect it to a fully connected layer with width =
  #   64.
  #   • A final fully connected layer with width = number of classes = 10 with softmax activation.
  clf = construct_classifier()

  # 5. Train this network with the following parameters:
  # • learning rate = 0.01
  # • optimizer = Adam
  # • number of epochs = 5
  # • loss function = sparse categorical cross-entropy.
  # • batch size = 64
  history = clf.fit( 
                  x=train_images,y=train_labels,epochs=5,batch_size=64,
                  validation_data=(test_images,test_labels) , shuffle=True,
                  # verbose=0,
                  workers=8,
                  use_multiprocessing=True
                )
  train_acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']

# 6. Create a plot of accuracy as a function of number of epoch for the training and test set.
  plt.rcParams["figure.figsize"] = (10,5)
  fig2,axs2 = plt.subplots(1,1)
  print("Plotting Accuracy VS Epoch")
  axs2.plot( np.arange(0,len(train_acc)),train_acc,label='Training Acc' )
  axs2.plot( np.arange(0,len(val_acc)),val_acc,label='Validation Acc' )
  axs2.set_xlabel("Epoch")
  axs2.legend()

  # 7. For the trained network extract the learned filters from the first convolutional layer. (Hint: you might
  # want to use combination of model.layers and Layer.get weights()). For each of the 32 filters of
  # the first layer, subtract their respective means and then plot all 32 filters of size (3 × 3). For each
  # subplot show the colorbar, and choose its limits symetrically about zero. Note the point of creating
  # these plots is to see if you can identify these filters (see the next task).
  plt.rcParams["figure.figsize"] = (25,20)
  fig3,axs3 = plt.subplots(4,8)
  layer0 = clf.layers[0]
  print("\n\n Plotting convolution matrices from first convolution \n\n")
  for i in range(4):
    for j in range(8):
      idx = i*8 + j
      conv = layer0.get_weights()[0][:,:,0,idx]
      conv -= np.mean(conv)
      fmax = conv.max()
      fmin = conv.min()
      if abs(fmax) > abs(fmin):
        fmin = - abs(fmax)
      else:
        fmax = abs(fmin)
      plt.colorbar(axs3[i,j].imshow(conv,
                                  vmin = fmin, vmax = fmax),ax=axs3[i,j], orientation='horizontal')
      axs3[i,j].axis("off")
      axs3[i,j].set_title( "Index: " + str(idx) )

"""Answer-8:

First Derivative:
Index: 24 and Index: 22 can be considered as the first derivative since central column is close to zero, right side is close to positive and left side is close to negative. So direction of taking the derivative would be +X

Second derivative:
Index: 19 in the above images are close to second derivative convolution since center value is negative and surrounding values are positive dominated. Direction of taking the derivative would be +X
"""